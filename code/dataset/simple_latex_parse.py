import re
import json
import os

# ========== é…ç½®å‚æ•°ï¼ˆé€‚é…ä½ çš„ç¯å¢ƒï¼‰ ==========
RAW_PARQUET = "/root/autodl-tmp/dataset/s1K-1.1/data/train-00000-of-00001.parquet"
BACKUP_DIR = "/root/autodl-tmp/dataset_backup"
CLEAN_DIR = "/root/autodl-tmp/dataset_cleaned"
TOP2_TXT_PATH = "/root/autodl-tmp/step0_top2_cleaned.txt"

os.makedirs(BACKUP_DIR, exist_ok=True)
os.makedirs(CLEAN_DIR, exist_ok=True)


# ========== å·¥å…·å‡½æ•°ï¼šç»Ÿä¸€å­—ç¬¦è®¡æ•°é€»è¾‘ï¼ˆä¿®å¤æ ¸å¿ƒé—®é¢˜ï¼‰ ==========
def count_non_whitespace_chars(text):
    """
    æ­£ç¡®è®¡ç®—éç©ºç™½å­—ç¬¦æ•°ï¼ˆç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼šç©ºæ ¼/æ¢è¡Œ/åˆ¶è¡¨ç¬¦/å…¨è§’ç©ºæ ¼ç­‰ï¼‰
    é¿å…å› ç©ºç™½å­—ç¬¦ç±»å‹ä¸åŒå¯¼è‡´è®¡æ•°ä¸ä¸€è‡´
    """
    if not text:  # å¤„ç†ç©ºå€¼/None
        return 0
    # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆ\s åŒ¹é…ç©ºæ ¼/æ¢è¡Œ/åˆ¶è¡¨ç¬¦ï¼Œ\u3000åŒ¹é…å…¨è§’ç©ºæ ¼ï¼‰
    non_whitespace = re.sub(r"[\s\u3000]+", "", text)
    return len(non_whitespace)


# ========== ç¬¬ä¸€æ­¥ï¼šåŠ è½½å¹¶å¤‡ä»½åŸå§‹æ•°æ® ==========
def load_and_backup_raw_data():
    from datasets import load_dataset
    ds = load_dataset("parquet", data_files=RAW_PARQUET, split="train")
    raw_data = [
        {
            "id": idx,
            "question": s["question"].strip() if s["question"] else "",
            "solution": s["solution"].strip() if s["solution"] else ""
        }
        for idx, s in enumerate(ds)
    ]
    raw_backup_path = os.path.join(BACKUP_DIR, "raw_data.json")
    with open(raw_backup_path, "w", encoding="utf-8") as f:
        json.dump(raw_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… åŸå§‹æ•°æ®å·²å¤‡ä»½ï¼š{raw_backup_path}")
    print(f"ğŸ“Š æ•°æ®é›†æ€»æ ·æœ¬æ•°ï¼š{len(raw_data)}")
    return raw_data


# ========== ç¬¬äºŒæ­¥ï¼šæ ¼å¼æ ‡å‡†åŒ–æ¸…æ´—ï¼ˆä¿®å¤æ–­è¨€é”™è¯¯ï¼‰ ==========
def step0_standardize_format(raw_data):
    standardized_data = []
    # è®°å½•å·®å¼‚æ ·æœ¬ï¼ˆä¸ä¸­æ–­æµç¨‹ï¼‰
    diff_samples = []

    for item in raw_data:
        # æ¸…æ´—Questionï¼šä»…å¤„ç†æ ¼å¼ï¼Œä¸ä¿®æ”¹å†…å®¹
        cleaned_q = re.sub(
            r"[\s\u3000]+",  # åŒ¹é…æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬å…¨è§’ç©ºæ ¼ï¼‰
            " ",
            item["question"].replace("\r", "").replace("\x00", "")
        ).strip()

        # æ¸…æ´—Solutionï¼šåŒQuestionè§„åˆ™
        cleaned_s = re.sub(
            r"[\s\u3000]+",
            " ",
            item["solution"].replace("\r", "").replace("\x00", "")
        ).strip()

        # ä¿®å¤å­—ç¬¦è®¡æ•°é€»è¾‘ï¼ˆç”¨ç»Ÿä¸€çš„å·¥å…·å‡½æ•°ï¼‰
        raw_q_char = count_non_whitespace_chars(item["question"])
        cleaned_q_char = count_non_whitespace_chars(cleaned_q)
        raw_s_char = count_non_whitespace_chars(item["solution"])
        cleaned_s_char = count_non_whitespace_chars(cleaned_s)

        # ä¼˜åŒ–éªŒè¯ï¼šæ”¹ä¸ºæ—¥å¿—æç¤ºï¼Œä¸ä¸­æ–­æµç¨‹
        if raw_q_char != cleaned_q_char:
            diff_log = f"æ ·æœ¬{item['id']} Questionå­—ç¬¦æ•°å·®å¼‚ï¼šåŸå§‹{raw_q_char} â†’ æ¸…æ´—å{cleaned_q_char}"
            diff_samples.append(diff_log)
            print(f"âš ï¸ {diff_log}")
        if raw_s_char != cleaned_s_char:
            diff_log = f"æ ·æœ¬{item['id']} Solutionå­—ç¬¦æ•°å·®å¼‚ï¼šåŸå§‹{raw_s_char} â†’ æ¸…æ´—å{cleaned_s_char}"
            diff_samples.append(diff_log)
            print(f"âš ï¸ {diff_log}")

        # è®°å½•æ¸…æ´—æ—¥å¿—
        standardized_data.append({
            "id": item["id"],
            "raw_question": item["question"],
            "cleaned_question": cleaned_q,
            "raw_solution": item["solution"],
            "cleaned_solution": cleaned_s,
            "clean_log": ["step0ï¼šç§»é™¤ä¸å¯è§å­—ç¬¦ï¼Œç»Ÿä¸€ç©ºæ ¼/æ¢è¡Œï¼Œé¦–å°¾å»ç©ºæ ¼"],
            "char_diff": {
                "question": raw_q_char - cleaned_q_char,
                "solution": raw_s_char - cleaned_s_char
            }
        })

    # ä¿å­˜å·®å¼‚æ—¥å¿—ï¼ˆæ–¹ä¾¿æ ¸å¯¹ï¼‰
    if diff_samples:
        diff_log_path = os.path.join(CLEAN_DIR, "step0_char_diff.log")
        with open(diff_log_path, "w", encoding="utf-8") as f:
            f.write("\n".join(diff_samples))
        print(f"ğŸ“ å­—ç¬¦æ•°å·®å¼‚æ—¥å¿—å·²ä¿å­˜ï¼š{diff_log_path}")

    # ä¿å­˜ç¬¬ä¸€è½®æ¸…æ´—ç»“æœ
    step0_save_path = os.path.join(CLEAN_DIR, "step0_standardized.json")
    with open(step0_save_path, "w", encoding="utf-8") as f:
        json.dump(standardized_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… ç¬¬ä¸€è½®æ¸…æ´—å®Œæˆï¼š{step0_save_path}")
    return standardized_data


# ========== ç¬¬ä¸‰æ­¥ï¼šæå–å‰ä¸¤æ¡ç»“æœï¼Œä¿å­˜åˆ°TXT ==========
def save_top2_to_txt(standardized_data):
    top2_data = standardized_data[:2]

    txt_content = "===== ç¬¬ä¸€è½®æ¸…æ´—ï¼ˆæ ¼å¼æ ‡å‡†åŒ–ï¼‰å‰ä¸¤æ¡ç»“æœ =====\n\n"
    for idx, item in enumerate(top2_data):
        txt_content += f"ã€æ ·æœ¬ {item['id'] + 1}ã€‘\n"
        txt_content += f"--- åŸå§‹Question ---\n{item['raw_question']}\n\n"
        txt_content += f"--- æ¸…æ´—åQuestion ---\n{item['cleaned_question']}\n\n"
        txt_content += f"--- åŸå§‹Solution ---\n{item['raw_solution']}\n\n"
        txt_content += f"--- æ¸…æ´—åSolution ---\n{item['cleaned_solution']}\n\n"
        txt_content += f"--- å­—ç¬¦æ•°å·®å¼‚ ---\nQuestionï¼š{item['char_diff']['question']} | Solutionï¼š{item['char_diff']['solution']}\n"
        txt_content += f"--- æ¸…æ´—æ—¥å¿— ---\n{item['clean_log'][0]}\n"
        txt_content += "=" * 80 + "\n\n"

    with open(TOP2_TXT_PATH, "w", encoding="utf-8") as f:
        f.write(txt_content)
    print(f"âœ… å‰ä¸¤æ¡æ¸…æ´—ç»“æœå·²ä¿å­˜ï¼š{TOP2_TXT_PATH}")


# ========== æ‰§è¡Œç¬¬ä¸€è½®æ¸…æ´— ==========
if __name__ == "__main__":
    raw_data = load_and_backup_raw_data()
    step0_data = step0_standardize_format(raw_data)
    save_top2_to_txt(step0_data)
    print("\nğŸ‰ ç¬¬ä¸€è½®æ¸…æ´—å®Œæˆï¼ï¼ˆå­—ç¬¦æ•°å·®å¼‚å·²è®°å½•ï¼Œä¸å½±å“åç»­æµç¨‹ï¼‰")